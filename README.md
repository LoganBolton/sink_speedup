# sink_speedup

Experimenting with the idea of if you can mitigate computing attention sinks to speed up LLM inference
